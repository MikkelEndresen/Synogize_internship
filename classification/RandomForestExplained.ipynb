{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4d07187-bab5-44f9-b913-33f8db328704",
   "metadata": {
    "name": "cell9",
    "collapsed": false
   },
   "source": "# Explainability of Random Forest Model\n\nInstall the follwoing packages:\n- numpy\n- pandas\n- scikit-learn\n- lime"
  },
  {
   "cell_type": "code",
   "id": "435de873-f708-444d-af23-09bdd8aa8ee9",
   "metadata": {
    "language": "python",
    "name": "cell10",
    "collapsed": false
   },
   "outputs": [],
   "source": "import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, recall_score, confusion_matrix\nimport time\nfrom sklearn.metrics import classification_report\nfrom lime.lime_tabular import LimeTabularExplainer\nfrom sklearn.inspection import PartialDependenceDisplay\nimport matplotlib.pyplot as plt\nimport shap",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f8773a7d-82f1-4ad1-a960-8b967e236807",
   "metadata": {
    "name": "cell1",
    "collapsed": false
   },
   "source": "## Setting up and training RandomForestClassification model"
  },
  {
   "cell_type": "code",
   "id": "543c5fea-3a4d-4ac9-b110-1481aff472c9",
   "metadata": {
    "language": "python",
    "name": "cell2"
   },
   "outputs": [],
   "source": "session = get_active_session()\n\nsession.use_database(\"ML\")\nsession.use_schema(\"RETAIL_STORE\")\n\ndf_training_data = session.table('training_data') # importing data\n\ndf_training_data = df_training_data.drop(\"CUSTOMER_ID\", \"OFFER_PRODUCT_ID\") # dropping id columns\nX = df_training_data.drop(\"REPEATER_INT\")\ny = df_training_data.select(\"REPEATER_INT\")\n\n\nFEATURE_COLS = X.columns[:len(X.columns)]\nLABEL_COLS = [\"REPEATER_INT\"]\n\nprint(f\"Feature Columns: {FEATURE_COLS}\")\n\nX = X.to_pandas()\ny = y.to_pandas()\n\ny = y.values.ravel()\n\n# 80/20 train/test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "64d40981-e798-45ff-91f3-660244f9480f",
   "metadata": {
    "name": "cell12",
    "collapsed": false
   },
   "source": "## Feature selection testing"
  },
  {
   "cell_type": "code",
   "id": "c4c864ac-0741-400d-9d4f-5b2584c4e99c",
   "metadata": {
    "language": "python",
    "name": "cell17",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Drop CHAIN_CAT_1, 2, and 3\n# Drop OFFER_VALUE 2, 3, 4\nX.drop(columns=['CHAIN_CAT_1', 'CHAIN_CAT_2', 'CHAIN_CAT_3', 'OFFER_VALUE_2', 'OFFER_VALUE_3', 'OFFER_VALUE_4'])\nprint(X.head())\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "37cc6c2b-9b9d-446c-9425-37b6917d6cf9",
   "metadata": {
    "name": "cell23",
    "collapsed": false
   },
   "source": "### Making TOTAL binary\n\nIf total > 5800, then 1 otherwise 0. \nResult:\n- Gives it a feature importance of 0.0"
  },
  {
   "cell_type": "code",
   "id": "3b8ebc79-3698-4a72-bed7-fa2603f2df17",
   "metadata": {
    "language": "python",
    "name": "cell22"
   },
   "outputs": [],
   "source": "# Making the total binary, above 5853 or not\nX['TOTAL'] = X['TOTAL'].apply(lambda x: 1 if x > 5800 else 0)\n\nprint(X['TOTAL'].head())\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f4f9e380-0370-4d2a-aacf-070b6b52f764",
   "metadata": {
    "name": "cell21",
    "collapsed": false
   },
   "source": "### Training model"
  },
  {
   "cell_type": "code",
   "id": "bc1f6f6f-ac71-4f53-a9a5-a5d4ac3da96f",
   "metadata": {
    "language": "python",
    "name": "cell3"
   },
   "outputs": [],
   "source": "start_time = time.time()\n\nmodel = RandomForestClassifier(\n    n_estimators = 50,\n    min_samples_split = 5,\n    min_samples_leaf = 1,\n    max_features = 'log2',\n    max_depth = 10, \n    class_weight = 'balanced'\n)\n\nmodel.fit(X_train, y_train)\n\nend_time = time.time()\ntraining_time = end_time - start_time",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "244cb412-4de3-4c7e-8a55-a08f52885d46",
   "metadata": {
    "name": "cell30",
    "collapsed": false
   },
   "source": "### Parameter optimisation example\n"
  },
  {
   "cell_type": "code",
   "id": "0c2675e9-2428-4ef8-bd73-a6a47c2d1802",
   "metadata": {
    "language": "python",
    "name": "cell24"
   },
   "outputs": [],
   "source": "from sklearn.model_selection import RandomizedSearchCV\n\nmodel = RandomForestClassifier()\n\n\nparam_dist = {\n    'n_estimators': [25, 50, 100, 200],\n    'max_depth': [None, 5, 10, 15, 20],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 5],\n    'max_features': ['sqrt', 'log2'], # dont use 'auto'. error\n}\n\n\nrandom_search = RandomizedSearchCV(\n    estimator=model,\n    param_distributions=param_dist,\n    n_iter=5, ###s\n    scoring='recall',\n    cv=5,\n    verbose=2,\n    random_state=42,\n)\n\nrandom_search.fit(X_train, y_train)\n\nend_time = time.time()\ntraining_time = end_time - start_time\n\nprint(\"Training time: \", training_time) \n\nparameters = random_search.best_params_\nprint(parameters)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "da586469-5b22-4d37-bae6-c6e5f3849b05",
   "metadata": {
    "name": "cell4",
    "collapsed": false
   },
   "source": "## Feature importance"
  },
  {
   "cell_type": "code",
   "id": "496c4fb2-a5b1-475c-8fb1-476ad7a6bce4",
   "metadata": {
    "language": "python",
    "name": "cell37"
   },
   "outputs": [],
   "source": "model = RandomForestClassifier(\n    n_estimators = 50,\n    min_samples_split = 5,\n    min_samples_leaf = 1,\n    max_features = 'log2',\n    max_depth = 10, # Changed\n    class_weight = 'balanced'\n)\n\nmodel.fit(X_train, y_train)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "61495431-d00f-4f29-b101-cfa3a94b1b97",
   "metadata": {
    "language": "python",
    "name": "cell5"
   },
   "outputs": [],
   "source": "\nimportances = model.feature_importances_\n\nFEATURE_COLS = X.columns.tolist()\n\nfeatures = dict(zip(FEATURE_COLS, importances))\n                \nimportances = dict(sorted(features.items(), key=lambda item: item[1], reverse=True))\n\nfor i, im in enumerate(importances.items()):\n    print(f\"{i+1}. Feature: {im[0]} - importance: {im[1]}\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "71499928-23cb-42a8-9523-8df15ced957f",
   "metadata": {
    "name": "cell6",
    "collapsed": false
   },
   "source": "## LIME\n\nLocal Interpretable Model-agnostic Explanations\n\n\nHow it works\n\nIt \"zooms\" in on a given date point and then creates new datapoints around it, a process called perturbation. Then it takes an easily explainable model such as a linear model and trains it on the new data points around. This then is used to inform how much each feature impacts the ouptut at various values. So for this model where there are no continous values, you can see to what degree each feature impacts the prediction depending on whether they are 0 or 1. "
  },
  {
   "cell_type": "code",
   "id": "1799b0d5-3972-4f67-8679-9a1e6c0a1eca",
   "metadata": {
    "language": "python",
    "name": "cell8"
   },
   "outputs": [],
   "source": "explainer = LimeTabularExplainer(\n    training_data=np.array(X_train),\n    feature_names=X_train.columns,\n    #categorical_features=X_train.columns,\n    mode='classification'\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d41cd4af-f2d5-44de-bc3e-d10e9fb54fcf",
   "metadata": {
    "language": "python",
    "name": "cell11"
   },
   "outputs": [],
   "source": "import warnings\nfrom random import randint\n\n# Ignore all warnings\nwarnings.filterwarnings('ignore')\n\n# Choose an instance to explain\ninstance_index = randint(0, 1500)\ninstance = X_test.iloc[instance_index]\n\nexplanation = explainer.explain_instance(\n    data_row=instance,\n    predict_fn=model.predict_proba\n)\n\nprint(f\"Instance index: {instance_index}\")\nexplanation.as_pyplot_figure()\n\n# 2000 < > 5853",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5ce6196c-040a-465e-a597-7bf2423f740d",
   "metadata": {
    "name": "cell7",
    "collapsed": false
   },
   "source": "## PDP\n\nWhat does it measure? For each feature it measures the average prediction if all data points were to assume that feature value. So for this model, it wil take a feature and assume it is 0 and then calculate the average probability of 1 across all data points, and then do the same for 1. A flat PDP indicates that the feature is not important, and the more it varies, the more improtant the feature is. Meaning that if there is a large difference in the prediction between the two values one of the feature takes, that feature has a large impact. \n\nThe plots below are linear because there is only two possible values on the x-axis, 0 and 1. If I was using continous variables in the model it would be a plotted non-linear(perhaps) line.  \n\nSeems more usable for continous variables as you can then see for what values there is a bigger change in the output. If you had age for example, there might be a marked change in your risk for cardiac disease when you hit 60. Easy to see with a PD plot. \n\nSlow to run with continous variables!"
  },
  {
   "cell_type": "code",
   "id": "e78ca936-181a-4b71-aa6b-e8d4d73d1ad2",
   "metadata": {
    "language": "python",
    "name": "cell15"
   },
   "outputs": [],
   "source": "print(FEATURE_COLS)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "548d4cb7-ca75-45ff-b1b7-f77f6d00ef1b",
   "metadata": {
    "language": "python",
    "name": "cell13",
    "collapsed": false
   },
   "outputs": [],
   "source": "FEATURE_COLS = X.columns.tolist()\ndisplay = PartialDependenceDisplay.from_estimator(model, X_train, FEATURE_COLS)\n\ndisplay.plot(pdp_lim={1: (0, 1)})\n\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0324a618-d87b-4c77-9c78-5747fcc23f4f",
   "metadata": {
    "language": "python",
    "name": "cell14"
   },
   "outputs": [],
   "source": "\n\nfor i, feature in enumerate(FEATURE_COLS):\n     plt.figure(figsize=(3, 3))\n     display = PartialDependenceDisplay.from_estimator(model, X_train, [feature])\n     display.plot(pdp_lim={0: (0, 1)})\n     plt.title(f'Partial Dependence Plot for {feature}')\n     plt.xlabel(feature)\n     plt.ylabel('Partial Dependence')\n     plt.grid(True)\n     plt.show()\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "16d92cd6-3ea6-461f-b910-6da7f931213a",
   "metadata": {
    "name": "cell18",
    "collapsed": false
   },
   "source": "## Storing model results\n"
  },
  {
   "cell_type": "code",
   "id": "57635776-cf63-4a7d-bfa8-3042f11254e1",
   "metadata": {
    "language": "python",
    "name": "cell20"
   },
   "outputs": [],
   "source": "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix\n\nstart_time = time.time()\n\npredictions = model.predict(X_test)\n\nend_time = time.time()\nprediction_time = end_time - start_time\n\nparameters = model.get_params()\n\naccuracy = accuracy_score(y_test, predictions)\nprint(f'Accuracy: {accuracy:.4f}')\n\nrecall = recall_score(y_test, predictions)\nprint(f'Recall: {recall:.4f}')\n\nconf_matrix = confusion_matrix(y_test, predictions)\nprint('Confusion Matrix:')\nprint(conf_matrix)\n\n# For storing in db\ntrue_positive = conf_matrix[0][0]  \ntrue_negative = conf_matrix[1][1]  \nfalse_positive = conf_matrix[0][1]  \nfalse_negative = conf_matrix[1][0]",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "17726941-7e1d-4744-9185-8cfc72c1dcce",
   "metadata": {
    "language": "python",
    "name": "cell16"
   },
   "outputs": [],
   "source": "import json\n\ndef record_performance(true_positive, true_negative, false_positive, false_negative, model_name, accuracy, recall, training_time, prediction_time, \n         parameters, coefficients, intercept, notes):\n\n    confusion_matrix_insert_sql = f\"\"\"\n        insert into model_results_schema.confusion_matrix\n        (true_positive, true_negative, false_positive, false_negative)\n        values\n        ({true_positive}, {true_negative}, {false_positive}, {false_negative});\n    \"\"\"\n    \n    session.sql(confusion_matrix_insert_sql).collect()\n\n    last_id_sql = \"\"\"\n        select id\n        from model_results_schema.confusion_matrix\n        order by create_at desc\n        limit 1;\n    \"\"\" \n\n    #SELECT LAST_VALUE(id) OVER (ORDER BY id RANGE BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS last_id\n    confusion_matrix_id = session.sql(last_id_sql).collect()\n    confusion_matrix_id = confusion_matrix_id[0]['ID']\n\n    # These two need to be on a string format.\n    if coefficients != \"\":\n        coefficients = ', '.join(map(str, coefficients))\n    parameters = json.dumps(parameters)\n    \n    # Insert data into the model_performance table\n    session.sql(f\"\"\"\n        insert into model_results_schema.model_performance\n            (model_name, accuracy, recall, confusion_matrix_id,\n            training_time, prediction_time, parameters, coefficients,\n            intercept, notes)\n        values\n            ('{model_name}', {accuracy}, {recall}, {confusion_matrix_id}, {training_time}, {prediction_time}, '{parameters}', '{coefficients}', {intercept}, '{notes}');\n    \"\"\").collect()\n\n    \n    return \"success\"\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "410bcd37-c350-4e42-8bf0-63301152566e",
   "metadata": {
    "language": "python",
    "name": "cell19"
   },
   "outputs": [],
   "source": "notes = \"depth=10 instead of 30\"\nmodel_name = \"RandomForestClassifier+total-depth10\"\n\nresult = record_performance(true_positive, true_negative, false_positive, false_negative, model_name, accuracy, recall, training_time, prediction_time, \n         parameters, [0], 0.0, notes)\nprint(result)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0201bb71-71ea-4f7d-971f-e240f8e0da54",
   "metadata": {
    "name": "cell25",
    "collapsed": false
   },
   "source": "## SHAP\n\nCalculating Shapley Values"
  },
  {
   "cell_type": "code",
   "id": "f3c6055c-3981-402e-9a82-727105cad0ef",
   "metadata": {
    "language": "python",
    "name": "cell31",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Small dataset\nX_small = X\ny_small = y\nX_train, X_test, y_train, y_test = train_test_split(X_small, y_small, test_size=0.2, random_state=42)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "57922ded-7fbd-4924-bba3-4ae7a5fb9121",
   "metadata": {
    "language": "python",
    "name": "cell32"
   },
   "outputs": [],
   "source": "start_time = time.time()\n\nmodel = RandomForestClassifier(\n    n_estimators = 50,\n    min_samples_split = 5,\n    min_samples_leaf = 1,\n    max_features = 'log2',\n    max_depth = 10, \n    class_weight = 'balanced'\n)\n\nmodel.fit(X_train, y_train)\n\nend_time = time.time()\ntraining_time = end_time - start_time",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3239ba18-0894-49db-a984-3157483238ea",
   "metadata": {
    "language": "python",
    "name": "cell26"
   },
   "outputs": [],
   "source": "explainer = shap.Explainer(model.predict, X_test)\n\nshap_values = explainer(X_test)\nshap_values",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4fee831e-ab0d-48ca-b1c4-8220cdacbe64",
   "metadata": {
    "language": "python",
    "name": "cell33"
   },
   "outputs": [],
   "source": "shap.plots.bar(shap_values)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "902b76de-5a62-40c4-94ce-5b8cd84beed1",
   "metadata": {
    "language": "python",
    "name": "cell27"
   },
   "outputs": [],
   "source": "shap.plots.beeswarm(shap_values)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c76ec149-9eeb-4567-8f00-4d4005502178",
   "metadata": {
    "name": "cell29",
    "collapsed": false
   },
   "source": "Bar plot\n- mean shap value"
  },
  {
   "cell_type": "code",
   "id": "68a64d7f-f420-4768-9785-8a0d1490ef08",
   "metadata": {
    "language": "python",
    "name": "cell28"
   },
   "outputs": [],
   "source": "shap.plots.bar(shap_values[23])",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "464a8dfa-2222-49a3-81ee-b857677a6809",
   "metadata": {
    "name": "cell35",
    "collapsed": false
   },
   "source": "### Note on SHAP in snowflake\n- Unable to display force plots\n- Unable to display dependence_plots, but scatter plots work."
  },
  {
   "cell_type": "code",
   "id": "c180a992-4d08-466e-886f-692aec39790b",
   "metadata": {
    "language": "python",
    "name": "cell36"
   },
   "outputs": [],
   "source": "shap.plots.scatter(shap_values[:,\"OFFER_VALUE_6\"], color=shap_values[:,\"OFFER_VALUE_6\"])",
   "execution_count": null
  }
 ]
}